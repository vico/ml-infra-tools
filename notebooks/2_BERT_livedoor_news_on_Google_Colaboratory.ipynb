{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYlVm0kpjx57"
   },
   "source": [
    "## 日本語BERTでlivedoorニュースを教師あり学習で分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IqIW6ar2Bm3J",
    "outputId": "44476f53-a1fc-4f50-e953-50441bef9c21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd95942a738>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乱数シードの固定\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED_VALUE = 1234  # これはなんでも良い\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ii-mqaAhCApI"
   },
   "source": [
    "### GPUの使用可能を確認\n",
    "\n",
    "画面上部のメニュー ランタイム > ランタイムのタイプを変更 で、 ノートブックの設定 を開く\n",
    "\n",
    "ハードウェアアクセラレータに GPU を選択し、 保存 する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W8TJgawCB_Nb",
    "outputId": "81b1d32b-d3f3-494b-f414-dd3d26c9c4b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPUの使用確認：True or False\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# TrueならGPU使用可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gc6dbVUfj1W-"
   },
   "source": [
    "## 準備1：Livedoorニュースをダウンロードしてtsvファイル化\n",
    "\n",
    "参考：https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/finetune-to-livedoor-corpus.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "colab_type": "code",
    "id": "8Vh0a49Gp-rS",
    "outputId": "1b327511-b985-44fa-899f-a23c308ffa6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'ldcc-20140209.tar.gz' already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Livedoorニュースのファイルをダウンロード\n",
    "!wget -nc \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "keY2WGdwjzLD",
    "outputId": "e62b13c3-aa74-4b49-cc7b-7cc36cf08e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sports-watch', 'README.txt', 'peachy', 'movie-enter', 'dokujo-tsushin', 'CHANGES.txt', 'livedoor-homme', 'topic-news', 'it-life-hack', 'kaden-channel', 'smax']\n",
      "カテゴリー数: 9\n",
      "['sports-watch', 'peachy', 'movie-enter', 'dokujo-tsushin', 'livedoor-homme', 'topic-news', 'it-life-hack', 'kaden-channel', 'smax']\n"
     ]
    }
   ],
   "source": [
    "# ファイルを解凍し、カテゴリー数と内容を確認\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# 解凍\n",
    "tar = tarfile.open(\"ldcc-20140209.tar.gz\", \"r:gz\")\n",
    "tar.extractall(\"./data/livedoor/\")\n",
    "tar.close()\n",
    "\n",
    "# フォルダのファイルとディレクトリを確認\n",
    "files_folders = [name for name in os.listdir(\"./data/livedoor/text/\")]\n",
    "print(files_folders)\n",
    "\n",
    "# カテゴリーのフォルダのみを抽出\n",
    "categories = [name for name in os.listdir(\n",
    "    \"./data/livedoor/text/\") if os.path.isdir(\"./data/livedoor/text/\"+name)]\n",
    "\n",
    "print(\"カテゴリー数:\", len(categories))\n",
    "print(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "Z201OQ7gvYOY",
    "outputId": "9766d63d-029f-4a2c-81b1-accb8ad0dffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0： http://news.livedoor.com/article/detail/6255260/\n",
      "\n",
      "1： 2012-02-07T09:00:00+0900\n",
      "\n",
      "2： 新しいヴァンパイアが誕生！　ジョニデ主演『ダーク・シャドウ』の公開日が決定\n",
      "\n",
      "3： 　こんなヴァンパイアは見たことがない！　ジョニー・デップとティム・バートン監督がタッグを組んだ映画『ダーク・シャドウズ（原題）』の邦題が『ダーク・シャドウ』に決定。日本公開日が5月19日に決まった。さらに、ジョニー・デップ演じるヴァンパイアの写真が公開された。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ファイルの中身を確認してみる\n",
    "file_name = \"./data/livedoor/text/movie-enter/movie-enter-6255260.txt\"\n",
    "\n",
    "with open(file_name, encoding='utf-8') as text_file:\n",
    "    text = text_file.readlines()\n",
    "    print(\"0：\", text[0])  # URL情報\n",
    "    print(\"1：\", text[1])  # タイムスタンプ\n",
    "    print(\"2：\", text[2])  # タイトル\n",
    "    print(\"3：\", text[3])  # 本文\n",
    "\n",
    "    # 今回は4要素目には本文は伸びていないが、4要素目以降に本文がある場合もある\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoKvaAK1vurV"
   },
   "outputs": [],
   "source": [
    "# 本文を取得する前処理関数を定義\n",
    "\n",
    "\n",
    "def extract_main_txt(file_name):\n",
    "    with open(file_name, encoding='utf-8') as text_file:\n",
    "        # 今回はタイトル行は外したいので、3要素目以降の本文のみ使用\n",
    "        text = text_file.readlines()[3:]\n",
    "\n",
    "        # 3要素目以降にも本文が入っている場合があるので、リストにして、後で結合させる\n",
    "        text = [sentence.strip() for sentence in text]  # 空白文字(スペースやタブ、改行)の削除\n",
    "        text = list(filter(lambda line: line != '', text))\n",
    "        text = ''.join(text)\n",
    "        text = text.translate(str.maketrans(\n",
    "            {'\\n': '', '\\t': '', '\\r': '', '\\u3000': ''}))  # 改行やタブ、全角スペースを消す\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq2ebKoOxThi"
   },
   "outputs": [],
   "source": [
    "# リストに前処理した本文と、カテゴリーのラベルを追加していく\n",
    "import glob\n",
    "\n",
    "list_text = []\n",
    "list_label = []\n",
    "\n",
    "for cat in categories:\n",
    "    text_files = glob.glob(os.path.join(\"./data/livedoor/text\", cat, \"*.txt\"))\n",
    "\n",
    "    # 前処理extract_main_txtを実施して本文を取得\n",
    "    body = [extract_main_txt(text_file) for text_file in text_files]\n",
    "\n",
    "    label = [cat] * len(body)  # bodyの数文だけカテゴリー名のラベルのリストを作成\n",
    "\n",
    "    list_text.extend(body)  # appendが要素を追加するのに対して、extendはリストごと追加する\n",
    "    list_label.extend(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "bKPb_LJxxuOM",
    "outputId": "51a0fb36-5377-4f79-8672-f89f8d88aa51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ロンドン五輪・柔道男子60kg級で銀メダルを獲得した平岡拓晃が、テレビ朝日の五輪特別番組に出演した。「メダルは金ではなかったんですが、4年間頑張ってきた色なのかなと思います」と切り出した平岡。準々決勝では、フランスのミルを相手に大接戦を繰り広げたが、「ポイントを取られていたので、前に出るしかないと思ってですね、抱きついてでもポイントを取ろうと思った。オリンピック前の練習でポイントを取られた時の練習も少しやっていたので、それが出たと思います」と振り返った。また、「冷静だった？」という問いには、「慌ててもいたんですが、やることはひとつだなと腹は決まっていた」と語る平岡は、一本勝ちを決めた準決勝についても、「ここ（準々決勝）で一回自分は死んだんだと。負けてる試合だったので、なんとか勝ちを拾ってそのときに、僕は死んだんだと思って、準決勝も決勝も開き直った」と話す。さらに、ガルスチャンに敗れた決勝は「掬い投げにいこうとしたんですけど、掬っていく方向を間違えてしまった。もう少し真上に引き上げてから掬えばよかったんですけど、はさまれてしまって。自分は前に出ているわけだから勢いで回ってしまって。判断ミスでした」と反省の弁を語った。そして、母や妻への感謝を口にした平岡は「自分の中では金メダル獲りたかったです」と改めて話しながらも、「とりあえず男子柔道は全員が金メダルを獲ることを目標にやってきて、僕が流れを作るべきだったんですけど、それができなくて。すぐ選手村に帰って、みんなに申し訳ないって頭を下げて。明日からは選手のサポートに回ることだけを考えています」と続けた。\n",
      "sports-watch\n"
     ]
    }
   ],
   "source": [
    "# 0番目の文章とラベルを確認\n",
    "print(list_text[0])\n",
    "print(list_label[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "a_Kf1D9xxuvP",
    "outputId": "f70a9ee6-28e7-40c6-8649-4ee921244805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7376, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ロンドン五輪・柔道男子60kg級で銀メダルを獲得した平岡拓晃が、テレビ朝日の五輪特別番組に出...</td>\n",
       "      <td>sports-watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4日放送、TBS「S1」では、「Weeklyストーブ 猛虎がMLBを語る」と題し、阪神タイガ...</td>\n",
       "      <td>sports-watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17日に行なわれたセリエA第24節で、日本代表DF長友佑都が所属するインテルはホームでボロー...</td>\n",
       "      <td>sports-watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5日、サッカー日本代表のアルベルト・ザッケローニ監督が宮市亮（ボルトン）を絶賛。「どんな性格...</td>\n",
       "      <td>sports-watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2日、大手スポーツ紙は、なでしこリーグ・INAC神戸による「異例のお願い」として、優勝が懸か...</td>\n",
       "      <td>sports-watch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         label\n",
       "0  ロンドン五輪・柔道男子60kg級で銀メダルを獲得した平岡拓晃が、テレビ朝日の五輪特別番組に出...  sports-watch\n",
       "1  4日放送、TBS「S1」では、「Weeklyストーブ 猛虎がMLBを語る」と題し、阪神タイガ...  sports-watch\n",
       "2  17日に行なわれたセリエA第24節で、日本代表DF長友佑都が所属するインテルはホームでボロー...  sports-watch\n",
       "3  5日、サッカー日本代表のアルベルト・ザッケローニ監督が宮市亮（ボルトン）を絶賛。「どんな性格...  sports-watch\n",
       "4  2日、大手スポーツ紙は、なでしこリーグ・INAC神戸による「異例のお願い」として、優勝が懸か...  sports-watch"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandasのDataFrameにする\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'text': list_text, 'label': list_label})\n",
    "\n",
    "# 大きさを確認しておく（7,376文章が存在）\n",
    "print(df.shape)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "kB8p83xi02ck",
    "outputId": "cdf990df-cf20-474e-8f40-6bdef266d772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'sports-watch', 1: 'peachy', 2: 'movie-enter', 3: 'dokujo-tsushin', 4: 'livedoor-homme', 5: 'topic-news', 6: 'it-life-hack', 7: 'kaden-channel', 8: 'smax'}\n",
      "{'sports-watch': 0, 'peachy': 1, 'movie-enter': 2, 'dokujo-tsushin': 3, 'livedoor-homme': 4, 'topic-news': 5, 'it-life-hack': 6, 'kaden-channel': 7, 'smax': 8}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ロンドン五輪・柔道男子60kg級で銀メダルを獲得した平岡拓晃が、テレビ朝日の五輪特別番組に出...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4日放送、TBS「S1」では、「Weeklyストーブ 猛虎がMLBを語る」と題し、阪神タイガ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17日に行なわれたセリエA第24節で、日本代表DF長友佑都が所属するインテルはホームでボロー...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5日、サッカー日本代表のアルベルト・ザッケローニ監督が宮市亮（ボルトン）を絶賛。「どんな性格...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2日、大手スポーツ紙は、なでしこリーグ・INAC神戸による「異例のお願い」として、優勝が懸か...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_index\n",
       "0  ロンドン五輪・柔道男子60kg級で銀メダルを獲得した平岡拓晃が、テレビ朝日の五輪特別番組に出...            0\n",
       "1  4日放送、TBS「S1」では、「Weeklyストーブ 猛虎がMLBを語る」と題し、阪神タイガ...            0\n",
       "2  17日に行なわれたセリエA第24節で、日本代表DF長友佑都が所属するインテルはホームでボロー...            0\n",
       "3  5日、サッカー日本代表のアルベルト・ザッケローニ監督が宮市亮（ボルトン）を絶賛。「どんな性格...            0\n",
       "4  2日、大手スポーツ紙は、なでしこリーグ・INAC神戸による「異例のお願い」として、優勝が懸か...            0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# カテゴリーの辞書を作成\n",
    "dic_id2cat = dict(zip(list(range(len(categories))), categories))\n",
    "dic_cat2id = dict(zip(categories, list(range(len(categories)))))\n",
    "\n",
    "print(dic_id2cat)\n",
    "print(dic_cat2id)\n",
    "\n",
    "# DataFrameにカテゴリーindexの列を作成\n",
    "df[\"label_index\"] = df[\"label\"].map(dic_cat2id)\n",
    "df.head()\n",
    "\n",
    "# label列を消去し、text, indexの順番にする\n",
    "df = df.loc[:, [\"text\", \"label_index\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "zaE_8vER18xY",
    "outputId": "189432c8-db85-488d-a616-b4c00f97eb71"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9日、ソウルの日本大使館正門に62歳の韓国人男性が、1トントラックで突っ込むという事件が起き...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>最近、新時代のモバイルノートと言われているUltrabook（ウルトラブック）の魅力的な新製...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>東日本大震災以降、節電が求められている。この夏、皆さんもさまざまな工夫をしていることでしょう...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>先日「リーダー」機能は意外に便利！ iOSのSafariを上手に使う方法【デジ通】で、Saf...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25日、大森南朋の口癖が「ナオ的には」であるという演劇関係者の談話をWeb版「女性自身」が報...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_index\n",
       "0  9日、ソウルの日本大使館正門に62歳の韓国人男性が、1トントラックで突っ込むという事件が起き...            5\n",
       "1  最近、新時代のモバイルノートと言われているUltrabook（ウルトラブック）の魅力的な新製...            6\n",
       "2  東日本大震災以降、節電が求められている。この夏、皆さんもさまざまな工夫をしていることでしょう...            3\n",
       "3  先日「リーダー」機能は意外に便利！ iOSのSafariを上手に使う方法【デジ通】で、Saf...            6\n",
       "4  25日、大森南朋の口癖が「ナオ的には」であるという演劇関係者の談話をWeb版「女性自身」が報...            5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 順番をシャッフルする\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "TRrt-XH72C3l",
    "outputId": "344d24a3-4b2d-4a7b-c9e2-daa538645b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1475, 2)\n",
      "(5901, 2)\n"
     ]
    }
   ],
   "source": [
    "# tsvファイルで保存する\n",
    "\n",
    "# 全体の2割の文章数\n",
    "len_0_2 = len(df) // 5\n",
    "\n",
    "# 前から2割をテストデータとする\n",
    "df[:len_0_2].to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
    "print(df[:len_0_2].shape)\n",
    "\n",
    "# 前2割からを訓練&検証データとする\n",
    "df[len_0_2:].to_csv(\"./train_eval.tsv\", sep='\\t', index=False, header=None)\n",
    "print(df[len_0_2:].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0NykW7u3Mw9"
   },
   "outputs": [],
   "source": [
    "# tsvファイルをダウンロードしたい場合\n",
    "# from google.colab import files\n",
    "\n",
    "# ダウンロードする場合はコメントを外す\n",
    "# 少し時間がかかる（4MB）\n",
    "# files.download(\"./test.tsv\")\n",
    "\n",
    "\n",
    "# ダウンロードする場合はコメントを外す\n",
    "# 少し時間がかかる（18MB）\n",
    "# files.download(\"./train_eval.tsv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPT3Pjr94oPW"
   },
   "source": [
    "## 準備2：LivedoorニュースをBERT用のDataLoaderにする\n",
    "\n",
    "Hugginfaceのリポジトリの案内とは異なり、torchtextを使用した手法で実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "kPXX4pr2-kY-",
    "outputId": "2a7d359e-0edd-4591-b13a-27e16ce8a075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.5.1 in /home/vjai/.local/lib/python3.6/site-packages (4.5.1)\n",
      "Requirement already satisfied: dataclasses in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (0.8)\n",
      "Requirement already satisfied: packaging in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (20.9)\n",
      "Requirement already satisfied: sacremoses in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (0.0.45)\n",
      "Requirement already satisfied: requests in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (1.19.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (0.10.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (2021.4.4)\n",
      "Requirement already satisfied: filelock in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (4.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vjai/.local/lib/python3.6/site-packages (from transformers==4.5.1) (4.60.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/vjai/.local/lib/python3.6/site-packages (from importlib-metadata->transformers==4.5.1) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vjai/.local/lib/python3.6/site-packages (from importlib-metadata->transformers==4.5.1) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vjai/.local/lib/python3.6/site-packages (from packaging->transformers==4.5.1) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/vjai/.local/lib/python3.6/site-packages (from requests->transformers==4.5.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vjai/.local/lib/python3.6/site-packages (from requests->transformers==4.5.1) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vjai/.local/lib/python3.6/site-packages (from requests->transformers==4.5.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/vjai/.local/lib/python3.6/site-packages (from requests->transformers==4.5.1) (4.0.0)\n",
      "Requirement already satisfied: joblib in /home/vjai/.local/lib/python3.6/site-packages (from sacremoses->transformers==4.5.1) (1.0.1)\n",
      "Requirement already satisfied: six in /home/vjai/.local/lib/python3.6/site-packages (from sacremoses->transformers==4.5.1) (1.16.0)\n",
      "Requirement already satisfied: click in /home/vjai/.local/lib/python3.6/site-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "# MeCabとtransformersの用意\n",
    "# !apt install aptitude swig\n",
    "# !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
    "# !pip install mecab-python3\n",
    "\n",
    "# !pip install transformers==4.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext in /home/vjai/.local/lib/python3.6/site-packages (0.9.1)\n",
      "Requirement already satisfied: requests in /home/vjai/.local/lib/python3.6/site-packages (from torchtext) (2.25.1)\n",
      "Requirement already satisfied: tqdm in /home/vjai/.local/lib/python3.6/site-packages (from torchtext) (4.60.0)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/vjai/.local/lib/python3.6/site-packages (from torchtext) (1.8.1)\n",
      "Requirement already satisfied: numpy in /home/vjai/.local/lib/python3.6/site-packages (from torchtext) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /home/vjai/.local/lib/python3.6/site-packages (from torch==1.8.1->torchtext) (3.10.0.0)\n",
      "Requirement already satisfied: dataclasses in /home/vjai/.local/lib/python3.6/site-packages (from torch==1.8.1->torchtext) (0.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/vjai/.local/lib/python3.6/site-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vjai/.local/lib/python3.6/site-packages (from requests->torchtext) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vjai/.local/lib/python3.6/site-packages (from requests->torchtext) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/vjai/.local/lib/python3.6/site-packages (from requests->torchtext) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch               1.8.1\n",
      "torchaudio          0.8.1\n",
      "torchtext           0.9.1\n",
      "torchvision         0.9.1\n",
      "transformers        4.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch\n",
    "!pip list | grep transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tKqo2TF9vzj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext  # torchtextを使用\n",
    "\n",
    "#from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "from transformers import BertJapaneseTokenizer\n",
    "# 日本語BERTの分かち書き用tokenizerです\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZypBWaE-PB6"
   },
   "outputs": [],
   "source": [
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "\n",
    "max_length = 512  # 東北大学_日本語版の最大の単語数（サブワード数）は512\n",
    "\n",
    "\n",
    "def tokenizer_512(input_text):\n",
    "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
    "    return tokenizer.encode(input_text, max_length=512, return_tensors='pt', truncation=True)[0]\n",
    "\n",
    "\n",
    "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False,\n",
    "                            include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
    "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
    "\n",
    "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# (注釈)：各引数を再確認\n",
    "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
    "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
    "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
    "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
    "# include_length: 文章の単語数のデータを保持するか\n",
    "# batch_first：ミニバッチの次元を用意するかどうか\n",
    "# fix_length：全部の文章をfix_lengthと同じ長さになるように、paddingします\n",
    "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyLNL-sd_Xd5"
   },
   "outputs": [],
   "source": [
    "# 各tsvファイルを読み込み、分かち書きをしてdatasetにします\n",
    "# 少し時間がかかります\n",
    "# train_eval：5901個、test：1475個\n",
    "dataset_train_eval, dataset_test = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path='.', train='train_eval.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "aNRofv_iDOYq",
    "outputId": "c4a58711-2136-4466-90b1-76b9241d2d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426\n",
      "1475\n",
      "1475\n"
     ]
    }
   ],
   "source": [
    "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
    "# train_eval：5901個、test：1475個\n",
    "\n",
    "dataset_train, dataset_eval = dataset_train_eval.split(\n",
    "    split_ratio=1.0 - 1475/5901, random_state=random.seed(1234))\n",
    "\n",
    "# datasetの長さを確認してみる\n",
    "print(dataset_train.__len__())\n",
    "print(dataset_eval.__len__())\n",
    "print(dataset_test.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "colab_type": "code",
    "id": "O_XilciAI2la",
    "outputId": "8a3d360a-a6aa-4bae-e653-e2863bf6e690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,    52,    53,  1141,     5, 10706,    11,  7123,    36,  5325,\n",
      "        28738, 28821, 27603, 28583, 13074, 28583,   416,    61,    17,  3081,\n",
      "        15031, 23049,   908,    38,     8,  1732,    95,    14,    41,    32,\n",
      "            7,   343,    20,    10,     8,  1732,  1996,   104,   197,    53,\n",
      "           14, 11802,     7,  5797,    11,  3723,    15,    16,  4481,     6,\n",
      "         8141,  3900,  8072, 28865, 12402, 28865,     5, 23851, 25071,    14,\n",
      "          176,   759,    11,  2372,    10,     8,  7168,     5,   622,    12,\n",
      "         2906,    26,    20,    16,    21,    10,     5,     9,     6,  8141,\n",
      "         7111,     5, 22825,  4187,     8,   306,     9,   654,    40,     5,\n",
      "         2523,    12,     9,    17,   247,    11,   906,    15,    16,    21,\n",
      "           10,    82,   759,    14,  3252,    26,    20,    16,    21,    10,\n",
      "            8,  6978,  2670,     7,     9,   197,    53,     5,   859,    48,\n",
      "           53,    14,  5294,     6,  3571,   813,    14, 28090,   187,    10,\n",
      "           13,  7105,  7111,    11,  1379,     7,  2834,    34,   140,  1559,\n",
      "           12,   759,    14, 12088,     8, 23851,    13, 22825,  4187,    14,\n",
      "        11358,    10,    82,     6, 26696,    15, 13153,    13,   297,  3571,\n",
      "         1151,     5,   859,     5,    52,    53,    12,    31,  7452,  6293,\n",
      "        28533,     5,  3028,    14,   759,    11,  7123,   140, 14587,    18,\n",
      "         1545,     7,    58,    10,    23,   171,     9,   929,  7452,   537,\n",
      "         2027,  3345,  6619,     7,  2392,    16,    21,    10,  5456, 11970,\n",
      "        28467,     5,  1454, 29034,  1031, 28690,     7,  3028,    11,  1901,\n",
      "           10,    82,     6,  7452,    14, 23851,    29, 22825,  4187,    11,\n",
      "        11681, 22723,   140,    45,     7,    58,    10,   258,   854,     6,\n",
      "         7452,    14, 23851,    11, 11681,     6,   759,    14,  5094,    10,\n",
      "            8,   171,     7,     6,  1920,    12,     9,    36,    23,  7452,\n",
      "           28, 23851,    28,    24,   552, 10742,    75,    40,  6172,  1270,\n",
      "        28610,     7, 18012,     5,    12,     9,    38,    64,   140,  1295,\n",
      "           14,  7441,    12,    33,     8,  3571,   362,     9, 18787,     7,\n",
      "          517,    16, 11633,  1549,    14,     6,   759,     7,   658, 28555,\n",
      "           11,  6734,     5,    28,  4326,   485,     8,  3788,    28,  7168,\n",
      "            5,   416,    17,    11, 16631,     7,  1298,    15,    16, 11633,\n",
      "         1549,   120,    75,     8, 25035,  1634,  2622,    35,  9680, 16188,\n",
      "         9594,  5961, 29141,    61, 12144,   833,    12,     5,  1232, 24450,\n",
      "           13,   634,  2150,    11,   450,    63,  7155,  1143,  2187, 22794,\n",
      "         1143,    65,    40,  5290,  9680,  3030, 10740, 28743,  2345,  9594,\n",
      "           35,    36,  4873, 21401,     5, 18116,    38,     7,   139,  2935,\n",
      "         5995, 18116, 11020,  1076, 29903,  4371, 28601,     5,  6712,    14,\n",
      "         2519,   679, 30657,  4459,  9594,    35,  2323, 29345,     5,   500,\n",
      "         3626,    76,   300, 15949,  6390,     6,  4884,     5,  5308,     5,\n",
      "         3028,     9,  2935, 30657,  3030, 10740, 28743,  2345,  9594,    35,\n",
      "        24596, 28512, 28504,    58,  3051, 28489,  2935, 14294, 18060, 13814,\n",
      "         7210,  9680,  4459,  9594,    35,  6369,    14, 22144, 13698, 28891,\n",
      "         1268,  4233,  1277,     5,   147, 15662, 28645,    11,  2442,   237,\n",
      "         9680,  3030, 10740, 28743,  2345,  9594,     3])\n",
      "長さ： 437\n",
      "ラベル： 7\n"
     ]
    }
   ],
   "source": [
    "# datasetの中身を確認してみる\n",
    "item = next(iter(dataset_train))\n",
    "print(item.Text)\n",
    "print(\"長さ：\", len(item.Text))  # 長さを確認 [CLS]から始まり[SEP]で終わる。512より長いと後ろが切れる\n",
    "print(\"ラベル：\", item.Label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "j7323f5aJFzz",
    "outputId": "709bd9d4-b4b5-47c7-d2a6-2b994085a861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '一', '人', '芸', 'の', '日本一', 'を', '決める', '「', '東洋', '##水', '##産', 'PRE', '##S', '##ENT', '##S', 'R', '-', '1', 'ぐ', '##らん', '##ぷり', '2012', '」', '。', '決勝', '戦', 'が', '20', '日', 'に', '行わ', 'れ', 'た', '。', '決勝', '進出', '者', '12', '人', 'が', '順番', 'に', 'ネタ', 'を', '披露', 'し', 'て', 'いき', '、', 'お笑い', 'コンビ', 'CO', '##W', '##CO', '##W', 'の', '多田', '健二', 'が', '初', '優勝', 'を', '決め', 'た', '。', '今回', 'の', '大会', 'で', '注目', 'さ', 'れ', 'て', 'い', 'た', 'の', 'は', '、', 'お笑い', '芸人', 'の', 'スギ', 'ちゃん', '。', '彼', 'は', '一般', 'から', 'の', '投票', 'で', 'は', '1', '位', 'を', '獲得', 'し', 'て', 'い', 'た', 'ため', '優勝', 'が', '期待', 'さ', 'れ', 'て', 'い', 'た', '。', 'ファイナル', 'ステージ', 'に', 'は', '12', '人', 'の', 'うち', '3', '人', 'が', '進み', '、', '審査', '委員', 'が', 'おもしろ', '##かっ', 'た', 'と', '思う', '芸人', 'を', '実際', 'に', '指名', 'する', 'という', '方法', 'で', '優勝', 'が', '決まる', '。', '多田', 'と', 'スギ', 'ちゃん', 'が', '引き分け', 'た', 'ため', '、', '採点', 'し', '直し', 'と', 'なり', '審査', '員', 'の', 'うち', 'の', '一', '人', 'で', 'ある', '木村', '祐', '##一', 'の', 'ポイント', 'が', '優勝', 'を', '決める', 'という', '劇的', 'な', '展開', 'に', 'なっ', 'た', '(', 'これ', 'は', '当初', '木村', 'のみ', '同じく', 'ファイ', '##ナリスト', 'に', '残っ', 'て', 'い', 'た', 'チュー', '##トリア', '##ル', 'の', '徳', '##井', '義', '##実', 'に', 'ポイント', 'を', '入れ', 'た', 'ため', '、', '木村', 'が', '多田', 'か', 'スギ', 'ちゃん', 'を', '選び', '直す', 'という', 'こと', 'に', 'なっ', 'た', ')。', '結果', '、', '木村', 'が', '多田', 'を', '選び', '、', '優勝', 'が', '決まっ', 'た', '。', 'これ', 'に', '、', 'ネット', 'で', 'は', '「', '(', '木村', 'も', '多田', 'も', ')', '同じ', '吉本', 'だ', 'から', 'って', '身', '##内', 'に', '甘い', 'の', 'で', 'は', '」', 'など', 'という', '声', 'が', '相次い', 'で', 'いる', '。', '審査', 'について', 'は', '公平', 'に', '行っ', 'て', 'もらい', 'たい', 'が', '、', '優勝', 'に', 'ケ', '##チ', 'を', 'つける', 'の', 'も', '悲', '##しい', '。', 'どちら', 'も', '今回', 'の', 'R', '1', 'を', 'バネ', 'に', '活躍', 'し', 'て', 'もらい', 'たい', 'もの', 'だ', '。', '■', '関連', '記事', '・', '【', 'セミナー', '】', 'NE', '##X', '-', 'FS', '100', 'で', 'の', '色', '創り', 'と', 'ライ', '##ティング', 'を', '映画', '『', '彩', '〜', 'a', '##ja', '〜', '』', 'から', '学ぶ', '【', 'ビデオ', 'SA', '##L', '##ON', '】', '・', '「', '傷', 'だらけ', 'の', 'ローラ', '」', 'に', 'なる', '?', 'ついに', 'ローラ', 'VS', '黒', '##柳', '徹', '##子', 'の', '対決', 'が', '実現', '!', '##【', '話題', '】', '・', '待', '##望', 'の', 'E', '##OS', '5', 'D', 'Mark', 'III', '、', '動画', 'の', '進化', 'の', 'ポイント', 'は', '?', '##【', 'ビデオ', 'SA', '##L', '##ON', '】', '・', 'おっ', '##き', '##く', 'なっ', 'ちゃ', '##う', '?', '次期', 'iPhone', 'にまつわる', '噂', '【', '話題', '】', '・', 'ソニー', 'が', 'Trans', '##fer', '##J', '##et', '規格', '対応', 'の', '新', 'LS', '##I', 'を', '商品', '化', '【', 'ビデオ', 'SA', '##L', '##ON', '】', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kaden-channel'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasetの中身を文章に戻し、確認\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(item.Text.tolist()))  # 文章\n",
    "dic_id2cat[int(item.Label)]  # id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qiBmmdsJ-aK"
   },
   "outputs": [],
   "source": [
    "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "batch_size = 16  # BERTでは16、32あたりを使用する\n",
    "\n",
    "dl_train = torchtext.legacy.data.Iterator(\n",
    "    dataset_train, batch_size=batch_size, train=True)\n",
    "\n",
    "dl_eval = torchtext.legacy.data.Iterator(\n",
    "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "dl_test = torchtext.legacy.data.Iterator(\n",
    "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "5-sNpiK5K14s",
    "outputId": "b2322c1e-977b-4728-82f0-1133716a035c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 16]\n",
      "\t[.Text]:('[torch.LongTensor of size 16x512]', '[torch.LongTensor of size 16]')\n",
      "\t[.Label]:[torch.LongTensor of size 16]\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderの動作確認 \n",
    "\n",
    "batch = next(iter(dl_test))\n",
    "print(batch)\n",
    "print(batch.Text[0].shape)\n",
    "print(batch.Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8e6NhQ3cLcq"
   },
   "source": [
    "## 準備3：BERTのクラス分類用のモデルを用意する\n",
    "\n",
    "Huggingfaceさんのをそのまま使うのではなく、BERTのbaseだけ使い、残りは自分で実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hFlvnI05a4xN",
    "outputId": "0c488a8d-1f1d-45c2-bfc8-f2b40e5e7fa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# from transformers.modeling_bert import BertModel\n",
    "from transformers import BertModel #BertForMaskedLM\n",
    "\n",
    "# BERTの日本語学習済みパラメータのモデルです\n",
    "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BgGd7fLPssV"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BertForLivedoor(nn.Module):\n",
    "    '''BERTモデルにLivedoorニュースの9クラスを判定する部分をつなげたモデル'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BertForLivedoor, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = model  # 日本語学習済みのBERTモデル\n",
    "\n",
    "        # headにポジネガ予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元768、出力は9クラス\n",
    "        self.cls = nn.Linear(in_features=768, out_features=9)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
    "\n",
    "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
    "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
    "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
    "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
    "        output = self.cls(vec_0)  # 全結合層\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IOtveynWRwK5",
    "outputId": "e142a63f-2d08-4614-ae02-b9221364c259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "net = BertForLivedoor()\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hkg7r2RIR7qU"
   },
   "source": [
    "## 準備4：BERTのファインチューニングの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPGYvX4RR3UT"
   },
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "\n",
    "# 1. まず全部を、勾配計算Falseにしてしまう\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
    "for param in net.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for param in net.cls.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Il_-wow4Suwe"
   },
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zu7KRn1bTIQp"
   },
   "source": [
    "## 5. 訓練を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNaAXgiITFiw"
   },
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BERTに入力\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            acc = (torch.sum(preds == labels.data)\n",
    "                                   ).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
    "                                iteration, loss.item(),  acc))\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JfnH-gAmS75e",
    "outputId": "e973a53b-f0ff-4b5c-f597-2afbef9f3837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "イテレーション 10 || Loss: 2.0011 || 10iter. || 本イテレーションの正解率：0.375\n",
      "イテレーション 20 || Loss: 2.0438 || 10iter. || 本イテレーションの正解率：0.3125\n",
      "イテレーション 30 || Loss: 1.4062 || 10iter. || 本イテレーションの正解率：0.6875\n",
      "イテレーション 40 || Loss: 1.2296 || 10iter. || 本イテレーションの正解率：0.5625\n",
      "イテレーション 50 || Loss: 1.5334 || 10iter. || 本イテレーションの正解率：0.5625\n",
      "イテレーション 60 || Loss: 1.0354 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 70 || Loss: 1.0237 || 10iter. || 本イテレーションの正解率：0.625\n",
      "イテレーション 80 || Loss: 0.9275 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 90 || Loss: 0.3846 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 100 || Loss: 0.5344 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 110 || Loss: 0.4774 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 120 || Loss: 0.5877 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 130 || Loss: 0.3691 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 140 || Loss: 0.7450 || 10iter. || 本イテレーションの正解率：0.6875\n",
      "イテレーション 150 || Loss: 0.6219 || 10iter. || 本イテレーションの正解率：0.6875\n",
      "イテレーション 160 || Loss: 0.5144 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 170 || Loss: 0.5735 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 180 || Loss: 0.8967 || 10iter. || 本イテレーションの正解率：0.625\n",
      "イテレーション 190 || Loss: 0.1598 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 200 || Loss: 0.7542 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 210 || Loss: 0.2548 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 220 || Loss: 0.3900 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 230 || Loss: 0.5199 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 240 || Loss: 0.3272 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 250 || Loss: 0.2786 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 260 || Loss: 0.4715 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 270 || Loss: 0.1965 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "Epoch 1/4 | train |  Loss: 0.8150 Acc: 0.7406\n",
      "Epoch 1/4 |  val  |  Loss: 0.3867 Acc: 0.8705\n",
      "イテレーション 10 || Loss: 0.0709 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 20 || Loss: 0.3861 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 30 || Loss: 0.0963 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 40 || Loss: 0.3256 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 50 || Loss: 0.1998 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 60 || Loss: 0.2290 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 70 || Loss: 0.4494 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 80 || Loss: 0.2066 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 90 || Loss: 0.1868 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 100 || Loss: 0.4137 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 110 || Loss: 0.1423 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 120 || Loss: 0.3566 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 130 || Loss: 0.2187 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 140 || Loss: 0.2312 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 150 || Loss: 0.1456 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 160 || Loss: 0.1479 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 170 || Loss: 0.3664 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 180 || Loss: 0.2511 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 190 || Loss: 0.5318 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 200 || Loss: 0.3662 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 210 || Loss: 0.4196 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 220 || Loss: 0.2825 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 230 || Loss: 0.4938 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 240 || Loss: 0.2695 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 250 || Loss: 0.1099 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 260 || Loss: 0.2780 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 270 || Loss: 0.2635 || 10iter. || 本イテレーションの正解率：0.875\n",
      "Epoch 2/4 | train |  Loss: 0.3202 Acc: 0.8952\n",
      "Epoch 2/4 |  val  |  Loss: 0.3059 Acc: 0.9078\n",
      "イテレーション 10 || Loss: 0.0873 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 20 || Loss: 0.1751 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 30 || Loss: 0.3739 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 40 || Loss: 0.2851 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 50 || Loss: 0.2607 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 60 || Loss: 0.1292 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 70 || Loss: 0.0825 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 80 || Loss: 0.2519 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 90 || Loss: 0.3979 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 100 || Loss: 0.5594 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 110 || Loss: 0.3502 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 120 || Loss: 0.0832 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 130 || Loss: 0.1169 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 140 || Loss: 0.1408 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 150 || Loss: 0.1389 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 160 || Loss: 0.7929 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 170 || Loss: 0.1159 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 180 || Loss: 0.2186 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 190 || Loss: 0.1511 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 200 || Loss: 0.2213 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 210 || Loss: 0.1059 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 220 || Loss: 0.2236 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 230 || Loss: 0.1735 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 240 || Loss: 0.1160 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 250 || Loss: 0.0660 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 260 || Loss: 0.3668 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 270 || Loss: 0.1319 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "Epoch 3/4 | train |  Loss: 0.2224 Acc: 0.9331\n",
      "Epoch 3/4 |  val  |  Loss: 0.2732 Acc: 0.9193\n",
      "イテレーション 10 || Loss: 0.0260 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 20 || Loss: 0.0892 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 30 || Loss: 0.4416 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 40 || Loss: 0.0284 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 50 || Loss: 0.0921 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 60 || Loss: 0.0513 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 70 || Loss: 0.4908 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 80 || Loss: 0.4012 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 90 || Loss: 0.0881 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 100 || Loss: 0.1577 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 110 || Loss: 0.0452 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 120 || Loss: 0.1906 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 130 || Loss: 0.0398 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 140 || Loss: 0.2328 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 150 || Loss: 0.0888 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 160 || Loss: 0.1077 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 170 || Loss: 0.0064 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 180 || Loss: 0.3788 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 190 || Loss: 0.1634 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 200 || Loss: 0.0483 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 210 || Loss: 0.0864 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 220 || Loss: 0.2804 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 230 || Loss: 0.0408 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 240 || Loss: 0.1340 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 250 || Loss: 0.1516 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 260 || Loss: 0.0279 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 270 || Loss: 0.0300 || 10iter. || 本イテレーションの正解率：1.0\n",
      "Epoch 4/4 | train |  Loss: 0.1627 Acc: 0.9498\n",
      "Epoch 4/4 |  val  |  Loss: 0.2772 Acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "# 学習・検証を実行する。1epochに2分ほどかかります\n",
    "num_epochs = 4\n",
    "net_trained = train_model(net, dataloaders_dict,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8zuZC_0yaOs"
   },
   "source": [
    "## テストデータでの性能を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "EdxKZzijT5PG",
    "outputId": "536411ea-c7e3-48c2-e60f-5c660127aea7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:27<00:00,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ1475個での正解率：0.9146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# テストデータでの正解率を求める\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "    labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # BertForLivedoorに入力\n",
    "        outputs = net_trained(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
    "\n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYvVlrl7y45g"
   },
   "source": [
    "https://yoheikikuta.github.io/bert-japanese/\n",
    "\n",
    "https://github.com/yoheikikuta/bert-japanese\n",
    "\n",
    "の「BERT with SentencePiece for Japanese text.」\n",
    "\n",
    "では、入力テキストにタイトルを含めていますが、今回はタイトルは除いています。\n",
    "\n",
    "同様にタイトルを抜いている、[BERTを用いた日本語文書分類タスクの学習・ハイパーパラメータチューニングの実践例](https://medium.com/karakuri/bert%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E6%97%A5%E6%9C%AC%E8%AA%9E%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%AE%E5%AD%A6%E7%BF%92-%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E5%AE%9F%E8%B7%B5%E4%BE%8B-2fa5e4299b16)でも、正解率が92%ちょっととなっており、ほぼ同じ正解率が得られました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Y37pHqN2YD0"
   },
   "source": [
    "以上。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2_BERT_livedoor_news_on_Google_Colaboratory.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
